{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The num_words parameter limits the vocabulary size to the top 10,000 most frequent words in the dataset.\n",
    "- Any word ranked below 10,000 in frequency is replaced with an out-of-vocabulary (OOV) token.\n",
    "- This helps in reducing computational complexity and ignoring rare words that might not contribute much to learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "(X_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 12500, 1: 12500}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "[1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 7944, 451, 202, 14, 6, 717]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])  # First movie review as a sequence of numbers\n",
    "print(x_test[0])  # Sentiment label for the first review (0 or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# convert the sequence of numbers to words\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict((value,key) for key,value in word_index.items())\n",
    "decoded_review = ' '.join([reverse_word_index.get(i-3,'?') for i in X_train[0]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here i-3 is used because \n",
    "\n",
    "The IMDB dataset reserves the first 3 indices for special tokens:\n",
    "\n",
    "0 → Padding (<PAD>)\n",
    "\n",
    "1 → Start of a sequence (<START>)\n",
    "\n",
    "2 → Unknown words (<UNK>)\n",
    "\n",
    "- So, when we retrieve words from reverse_word_index, we shift the indices back by 3 to match the actual words in word_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  591,  202, ...,    0,    0,    0],\n",
       "       [   1,   14,   22, ...,    0,    0,    0],\n",
       "       [  33,    6,   58, ...,    9,   57,  975],\n",
       "       ...,\n",
       "       [   1,   13, 1408, ...,    0,    0,    0],\n",
       "       [   1,   11,  119, ...,    0,    0,    0],\n",
       "       [   1,    6,   52, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now adding padding to the each review to make them of same length\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "max_len = 500\n",
    "X_train = sequence.pad_sequences(X_train,maxlen=max_len,padding='post')\n",
    "x_test = sequence.pad_sequences(x_test,maxlen=max_len,padding='post')\n",
    "# X_train\n",
    "x_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use of each layer in the model:\n",
    "- Embedding layer: This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding).\n",
    "- SimpleRNN layer: This layer processes the sequence of word vectors. It takes the sequence of word embeddings as input and returns the output for each word in the sequence. The output of the SimpleRNN layer is the hidden state for each word in the sequence.\n",
    "- Dense layer: This layer processes the hidden state from the SimpleRNN layer and returns the final output. The output is a single prediction for the entire sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the simple rnn model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense,Dropout ,BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(max_features, 128, input_length=max_len),\n",
    "    Dropout(0.3),  # Prevent overfitting\n",
    "    SimpleRNN(128, activation='tanh'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 500, 128)          1280000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 128)          0         \n",
      "                                                                 \n",
      " simple_rnn_4 (SimpleRNN)    (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1313537 (5.01 MB)\n",
      "Trainable params: 1313281 (5.01 MB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding callbacks for early stopping and tensorboard\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "earlyStoppingCallback = EarlyStopping(patience=5,restore_best_weights=True,monitor='val_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# tensor board callback\n",
    "log_dir = 'classification_logs/fit/' + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir= log_dir,histogram_freq =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 195s 303ms/step - loss: 0.7045 - accuracy: 0.4989 - val_loss: 0.8572 - val_accuracy: 0.4938\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 142s 227ms/step - loss: 0.6861 - accuracy: 0.5134 - val_loss: 2.5345 - val_accuracy: 0.4938\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 136s 218ms/step - loss: 0.6726 - accuracy: 0.5307 - val_loss: 0.7440 - val_accuracy: 0.5140\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 135s 217ms/step - loss: 0.6632 - accuracy: 0.5343 - val_loss: 6.5961 - val_accuracy: 0.4938\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 136s 218ms/step - loss: 0.6672 - accuracy: 0.5340 - val_loss: 1.0157 - val_accuracy: 0.5062\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 134s 215ms/step - loss: 0.6648 - accuracy: 0.5322 - val_loss: 0.7013 - val_accuracy: 0.5018\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 140s 224ms/step - loss: 0.6545 - accuracy: 0.5408 - val_loss: 0.7038 - val_accuracy: 0.5018\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 138s 220ms/step - loss: 0.6514 - accuracy: 0.5397 - val_loss: 0.7074 - val_accuracy: 0.5024\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 136s 217ms/step - loss: 0.6470 - accuracy: 0.5459 - val_loss: 0.7076 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 137s 220ms/step - loss: 0.6466 - accuracy: 0.5474 - val_loss: 0.7072 - val_accuracy: 0.5040\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,y_train,epochs=10,batch_size=32, validation_split=0.2 ,callbacks = [tensorboard_callback,earlyStoppingCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('imdb_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dc5b6809417c8b3a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dc5b6809417c8b3a\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir regression_logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
